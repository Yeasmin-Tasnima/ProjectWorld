{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_c9edd4732    1\n",
       "id_95ba069bd    1\n",
       "id_b39e51fe1    1\n",
       "id_6a5ff9eca    1\n",
       "id_db66cbf23    1\n",
       "               ..\n",
       "id_41cea8a22    1\n",
       "id_26fdd51d8    1\n",
       "id_b46e9cf3a    1\n",
       "id_a111fe002    1\n",
       "id_69fd62552    1\n",
       "Name: sig_id, Length: 23814, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = pd.read_csv('data/train_features.csv')\n",
    "train_targets = pd.read_csv('data/train_targets_scored.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_features\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n",
    "\n",
    "X = df.drop(df.columns[0], axis=1).values\n",
    "X = X.astype('float32')\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    \"\"\"\n",
    "    Find the underrepresented targets.\n",
    "    Underrepresented targets are those which are observed less than the median occurance.\n",
    "    Targets beyond a quantile limit are filtered.\n",
    "    \"\"\"\n",
    "    irlbl = df.sum(axis=0)\n",
    "    print(irlbl.shape)\n",
    "    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    print(irlbl.max())\n",
    "    print(irlbl.shape)\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    print(threshold_irlbl)\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    print(len(tail_label))\n",
    "    return tail_label\n",
    "\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    \"\"\"\n",
    "    return\n",
    "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
    "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
    "    \"\"\"\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    \n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    print(X_sub.shape, y_sub.shape)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    \"\"\"\n",
    "    Give index of 10 nearest neighbor of all the instance\n",
    "    \n",
    "    args\n",
    "    X: np.array, array whose nearest neighbor has to find\n",
    "    \n",
    "    return\n",
    "    indices: list of list, index of 5 NN of each element in X\n",
    "    \"\"\"\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adasyn(X, y, beta, K, threshold=1):\n",
    "\n",
    "    \"\"\"\n",
    "    Adaptively generating minority data samples according to their distributions.\n",
    "    More synthetic data is generated for minority class samples that are harder to learn.\n",
    "    Harder to learn data is defined as positive examples with not many examples for in their respective neighbourhood.\n",
    "    Inputs\n",
    "         -----\n",
    "         X:  Input features, X, sorted by the minority examples on top.  Minority example should also be labeled as 1\n",
    "         y:  Labels, with minority example labeled as 1\n",
    "      beta:  Degree of imbalance desired.  Neg:Pos. A 1 means the positive and negative examples are perfectly balanced.\n",
    "         K:  Amount of neighbours to look at\n",
    " threshold:  Amount of imbalance rebalance required for algorithm\n",
    "    Variables\n",
    "         -----\n",
    "         xi:  Minority example\n",
    "        xzi:  A minority example inside the neighbourhood of xi\n",
    "         ms:  Amount of data in minority class\n",
    "         ml:  Amount of data in majority class\n",
    "        clf:  k-NN classifier model\n",
    "          d:  Ratio of minority : majority\n",
    "       beta:  Degree of imbalance desired\n",
    "          G:  Amount of data to generate\n",
    "         Ri:  Ratio of majority data / neighbourhood size.  Larger ratio means the neighbourhood is harder to learn,\n",
    "              thus generating more data.\n",
    "     Minority_per_xi:  All the minority data's index by neighbourhood\n",
    "     Rhat_i:  Normalized Ri, where sum = 1\n",
    "         Gi:  Amount of data to generate per neighbourhood (indexed by neighbourhoods corresponding to xi)\n",
    "    Returns\n",
    "         -----\n",
    "  syn_data:  New synthetic minority data created\n",
    "    \"\"\"\n",
    "\n",
    "    ms = int(sum(y))\n",
    "    ml = len(y) - ms\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Step 1, calculate the degree of class imbalance.  If degree of class imbalance is violated, continue.\n",
    "    d = np.divide(ms, ml)\n",
    "\n",
    "    if d > threshold:\n",
    "        return print(\"The data set is not imbalanced enough.\")\n",
    "\n",
    "    # Step 2a, if the minority data set is below the maximum tolerated threshold, generate data.\n",
    "    # Beta is the desired balance level parameter.  Beta > 1 means u want more of the imbalanced type, vice versa.\n",
    "    G = (ml - ms) * beta\n",
    "\n",
    "    # Step 2b, find the K nearest neighbours of each minority class example in euclidean distance.\n",
    "    # Find the ratio ri = majority_class in neighbourhood / K\n",
    "    Ri = []\n",
    "    Minority_per_xi = []\n",
    "    for i in range(ms):\n",
    "        xi = X[i, :].reshape(1, -1)\n",
    "        # Returns indices of the closest neighbours, and return it as a list\n",
    "        neighbours = clf.kneighbors(xi, n_neighbors=K, return_distance=False)[0]\n",
    "        # Skip classifying itself as one of its own neighbours\n",
    "        # neighbours = neighbours[1:]\n",
    "\n",
    "        # Count how many belongs to the majority class\n",
    "        count = 0\n",
    "        for value in neighbours:\n",
    "            if value > ms:\n",
    "                count += 1\n",
    "\n",
    "        Ri.append(count / K)\n",
    "\n",
    "        # Find all the minority examples\n",
    "        minority = []\n",
    "        for value in neighbours:\n",
    "            # Shifted back 1 because indices start at 0\n",
    "            if value <= ms - 1:\n",
    "                minority.append(value)\n",
    "\n",
    "        Minority_per_xi.append(minority)\n",
    "\n",
    "    # Step 2c, normalize ri's so their sum equals to 1\n",
    "    Rhat_i = []\n",
    "    for ri in Ri:\n",
    "        rhat_i = ri / sum(Ri)\n",
    "        Rhat_i.append(rhat_i)\n",
    "\n",
    "    assert(sum(Rhat_i) > 0.99)\n",
    "\n",
    "    # Step 2d, calculate the number of synthetic data examples that will be generated for each minority example\n",
    "    Gi = []\n",
    "    for rhat_i in Rhat_i:\n",
    "        gi = round(rhat_i * G)\n",
    "        Gi.append(int(gi))\n",
    "\n",
    "    # # Step 2e, generate synthetic examples\n",
    "    syn_data = []\n",
    "    for i in range(ms):\n",
    "        xi = X[i, :].reshape(1, -1)\n",
    "        for j in range(Gi[i]):\n",
    "            # If the minority list is not empty\n",
    "            if Minority_per_xi[i]:\n",
    "                index = np.random.choice(Minority_per_xi[i])\n",
    "                xzi = X[index, :].reshape(1, -1)\n",
    "                si = xi + (xzi - xi) * np.random.uniform(0, 1)\n",
    "                syn_data.append(si)\n",
    "\n",
    "    # Test the new generated data\n",
    "    test = []\n",
    "    for values in syn_data:\n",
    "        a = clf.predict(values)\n",
    "        test.append(a)\n",
    "\n",
    "    print(\"Using the old classifier, {} out of {} would be classified as minority.\".format(np.sum(test), len(syn_data)))\n",
    "\n",
    "    # Build the data matrix\n",
    "    data = []\n",
    "    for values in syn_data:\n",
    "        data.append(values[0])\n",
    "\n",
    "    print(\"{} amount of minority class samples generated\".format(len(data)))\n",
    "\n",
    "    # Concatenate the positive labels with the newly made data\n",
    "    labels = np.ones([len(data), 1])\n",
    "    data = np.concatenate([labels, data], axis=1)\n",
    "\n",
    "    # Concatenate with old data\n",
    "    org_data = np.concatenate([y.reshape(-1, 1), X], axis=1)\n",
    "    data = np.concatenate([data, org_data])\n",
    "\n",
    "    return data, Minority_per_xi, Ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOLTE_cat_wrapper(x_df, y_df, cat_col, nsamples):\n",
    "    x_df_up = pd.DataFrame(columns=x_df.columns)\n",
    "    y_df_up = pd.DataFrame(columns=y_df.columns)\n",
    "\n",
    "    unique_cat_combs = x_df.groupby(cat_col).size().reset_index().rename(columns={0:'count'})[cat_col]\n",
    "    num_cols = x_df.columns.drop(cat_col).tolist()\n",
    "    for index, row in unique_cat_combs.iterrows():\n",
    "        condition = (x_df[cat_col] == row).all(axis=1)\n",
    "\n",
    "        subx = x_df[condition][num_cols].reset_index(drop=True)\n",
    "        suby = y_df[condition].reset_index(drop=True)\n",
    "        print(subx.shape, suby.shape)\n",
    "\n",
    "        x_df_sub, y_df_sub = get_minority_samples(subx, suby)\n",
    "        Syn_data, neighbourhoods, Ri = adasyn(X, y, beta=0.05, K=15, threshold=1)\n",
    "        np.savetxt(path + 'data/syn_beta_0.05_k_15.csv', Syn_data, delimiter=',')\n",
    "        return\n",
    "        a, b = MLSMOTE(x_df_sub, y_df_sub, nsamples, neigh=10)\n",
    "        cats = pd.concat([row.to_frame().T]*len(a), ignore_index=True)\n",
    "        a = pd.merge(cats, a, how='left', left_index=True, right_index=True)\n",
    "        x_df_up = x_df_up.append(a, ignore_index=True)\n",
    "        y_df_up = y_df_up.append(b, ignore_index=True)\n",
    "    #y_df_up = y_df_up.astype(int)\n",
    "    \n",
    "    print('Number of new samples created: %d' %(len(y_df_up)))\n",
    "    \n",
    "    x_df_up = pd.concat([x_df, x_df_up], ignore_index=True)\n",
    "    y_df_up = pd.concat([y_df, y_df_up], ignore_index=True)\n",
    "    \n",
    "    x_df_up = x_df_up.sample(len(x_df_up), random_state=1881).reset_index(drop=True)\n",
    "    y_df_up = y_df_up.sample(len(y_df_up), random_state=1881).reset_index(drop=True)\n",
    "    \n",
    "    x_df_up[cat_col] = x_df_up[cat_col].astype(int)\n",
    "    return x_df_up, y_df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train sample size: (23814, 876) , Original Train target size: (23814, 207)\n",
      "(3886, 873) (3886, 206)\n",
      "(206,)\n",
      "122\n",
      "(183,)\n",
      "15.25\n",
      "89\n",
      "(377, 873) (377, 206)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b0a57fe1a7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcat_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cp_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cp_dose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOLTE_cat_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-fee4bede5533>\u001b[0m in \u001b[0;36mSMOLTE_cat_wrapper\u001b[0;34m(x_df, y_df, cat_col, nsamples)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_df_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_df_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minority_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mSyn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbourhoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madasyn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data/syn_beta_0.05_k_15.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9f7ccfa69593>\u001b[0m in \u001b[0;36madasyn\u001b[0;34m(X, y, beta, K, threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "train_features = pd.read_csv('data/train_features.csv')\n",
    "train_target = pd.read_csv('data/train_targets_scored.csv')\n",
    "print(\"Original Train sample size:\", train_features.shape, \", Original Train target size:\", train_target.shape)\n",
    "\n",
    "train_features = preprocess_df(train_features)\n",
    "\n",
    "train_features = train_features.drop('sig_id', axis=1)\n",
    "train_target = train_target.drop('sig_id', axis=1)\n",
    "\n",
    "cat_col = ['cp_time', 'cp_dose']\n",
    "x_train_fold, y_train_fold = SMOLTE_cat_wrapper(train_features, train_target, cat_col, nsamples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-35de58b68338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSyn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbourhoods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madasyn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data/syn_beta_0.05_k_15.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cbf365ec6ceb>\u001b[0m in \u001b[0;36madasyn\u001b[0;34m(X, y, beta, K, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madasyn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "Syn_data, neighbourhoods, Ri = adasyn(X, y, beta=0.05, K=15, threshold=1)\n",
    "np.savetxt(path + 'data/syn_beta_0.05_k_15.csv', Syn_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
